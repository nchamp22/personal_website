[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Science Blog: Working with Data in Pandas Tutorial",
    "section": "",
    "text": "In this tutorial, you will learn how to import a dataset, filter the data, and create visualizations using Pandas to explore and analyze it. Specifically, we will work with a dataset containing U.S. cancer incidence counts (1999–2022), grouped by race, cancer site, age group, and sex."
  },
  {
    "objectID": "blog.html#overview",
    "href": "blog.html#overview",
    "title": "Data Science Blog",
    "section": "",
    "text": "Write a brief description of the project here."
  },
  {
    "objectID": "blog.html#details",
    "href": "blog.html#details",
    "title": "Data Science Blog",
    "section": "",
    "text": "What I did\nTools used\nResults"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! This site is part of Brigham Young University’s STAT 386 Data Science Process class and showcases my journey in learning data science and analytics. As you explore this site, you’ll get to know a little more about my background, see some of the projects I am working on, and learn about the skills I am developing.\n\n\n\nProgramming: Python (including Pandas and Numpy)\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data\n\n\n\n\nI built this site using Quarto and host it on GitHub Pages.\n\n\n\n\n\n\nLearn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "index.html#about-this-portfolio",
    "href": "index.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "I built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#skills-im-learning",
    "href": "index.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python (including Pandas and Numpy)\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data"
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Learn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "Data Acquisition Blog",
    "section": "",
    "text": "This blog post showcases how I used the Python package Beautiful Soup to curate a custom dataset. Finding a website to scrape can be challenging, as many sites with interesting data—such as Rate My Professors and other review platforms—disallow scraping.\nI ultimately decided to scrape the Book of Mormon, found on the Church of Jesus Christ of Latter-day Saints’ website. My goal was to gather data on each chapter, including the chapter summary, the average verse length per chapter, and the most frequent word in each chapter. I thought this would be interesting both as a personal project and as an example for others wanting to learn how to use Beautiful Soup to scrape other HTML-based texts.",
    "crumbs": [
      "Data Acquisition Blog"
    ]
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "EDA Project"
    ]
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an Applied Statistics major at Brigham Young University. I was born and raised in Canada, but I have also lived in Japan, the Philippines, and the United States. I am happily married to my beautiful wife, and we have a 4-month-old son named Leo. I love learning new things, and you can often find me reading. I have been training in jiujitsu since I was eleven years old, and I also enjoy skiing and spending time playing with my son."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "",
    "text": "I am an Applied Statistics Major at Brigham Young University. I love using data to solve real problems."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nStatistics (BS): Applied Statistics & Analytics Emphasis - Brigham Young University, Sept. 2024 - PRESENT"
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python, R\nData Analysis: Pandas, NumPy, Excel\nVisualization: Matplotlib, Seaborn\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\n\nPublic Health Data\nSocioeconomic Data"
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\n\nWork on projects related to finance and public health\nLearn more machine learning techniques"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: nchamp@byu.du\nGitHub: github.com/nchamp22\nLinkedIn: linkedin.com/in/noah-champagne"
  },
  {
    "objectID": "blog.html#working-with-data-in-pandas",
    "href": "blog.html#working-with-data-in-pandas",
    "title": "Data Science Blog:",
    "section": "",
    "text": "In this tutorial, readers will learn how to import a dataset, clean and filter the data, and create visualizations using Pandas to explore and analyze it. Specifically, they will work with a dataset containing US cancer incidence rates (1999–2022), grouped by race, cancer site, age group, and sex, but the Pandas techniques demonstrated can be applied to any dataset they choose."
  },
  {
    "objectID": "blog.html#introduction-to-the-data",
    "href": "blog.html#introduction-to-the-data",
    "title": "Data Science Blog: Working with Data in Pandas Tutorial",
    "section": "Introduction to the Data",
    "text": "Introduction to the Data\nAs mentioned in the blog introduction, we will be working with a dataset containing U.S. cancer incidence counts from 1999 to 2022.\nThe dataset includes the following variables:\n\nRace: American Indian or Alaska Native, Asian or Pacific Islander, Black or African American, White, and Other.\nCancer site: The location of the cancer, such as stomach, small intestine, tonsil, salivary gland, and others.\nAge: Grouped into 4-year intervals from &lt;1 in some cases to 85+ (e.g., 20–24, 25–29, etc.).\nRates: Calculated per 100,000 people.\n\nThe dataset contains 12,532 rows and represents a subset of all cancer data collected during this period.\n\nUnited States and Puerto Rico Cancer Statistics, 1999-2022 Incidence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRace\nCode\nCancer Site\nSite Code\nAge Group\nAge Code\nSex\nSex Code\nCount\n\n\n\n\nAsian or Pacific Islander\nA-PI\nLung and Bronchus\n22030\n85+ years\n85\nFemale\nF\n5305\n\n\nAsian or Pacific Islander\nA-PI\nLung and Bronchus\n22030\n85+ years\n85\nMale\nM\n5773\n\n\nAsian or Pacific Islander\nA-PI\nTrachea, Mediastinum and Other Respiratory Organs\n22060\n1-4 years\n1-4\nFemale\nF\n16\n\n\nAsian or Pacific Islander\nA-PI\nTrachea, Mediastinum and Other Respiratory Organs\n22060\n15-19 years\n15-19\nMale\nM\n53"
  },
  {
    "objectID": "blog.html#loading-in-the-data",
    "href": "blog.html#loading-in-the-data",
    "title": "Data Science Blog: Working with Data in Pandas Tutorial",
    "section": "Loading in the Data",
    "text": "Loading in the Data\nNow that we understand what we are working with, we can load in the data. In Pandas, we can read in a CSV file with the following code (click on the dropdown arrow beside the word code to see the code and comments that explain how the code works):\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"cancer.csv\") ## this reads in the CSV\ndf.head() ## this shows the first five rows\n\n\n\n\n\n\n\n\n\nNotes\nRace\nRace Code\nCancer Sites\nCancer Sites Code\nAge Groups\nAge Groups Code\nSex\nSex Code\nCount\n\n\n\n\n0\nNaN\nAmerican Indian or Alaska Native\n1002-5\nAll Invasive Cancer Sites Combined\n0\n&lt; 1 year\n1\nFemale\nF\n123.0\n\n\n1\nNaN\nAmerican Indian or Alaska Native\n1002-5\nAll Invasive Cancer Sites Combined\n0\n&lt; 1 year\n1\nMale\nM\n139.0\n\n\n2\nNaN\nAmerican Indian or Alaska Native\n1002-5\nAll Invasive Cancer Sites Combined\n0\n1-4 years\n1-4\nFemale\nF\n378.0\n\n\n3\nNaN\nAmerican Indian or Alaska Native\n1002-5\nAll Invasive Cancer Sites Combined\n0\n1-4 years\n1-4\nMale\nM\n520.0\n\n\n4\nNaN\nAmerican Indian or Alaska Native\n1002-5\nAll Invasive Cancer Sites Combined\n0\n5-9 years\n5-9\nFemale\nF\n288.0"
  },
  {
    "objectID": "blog.html#cleaning-and-filtering-the-data",
    "href": "blog.html#cleaning-and-filtering-the-data",
    "title": "Data Science Blog: Working with Data in Pandas",
    "section": "Cleaning and Filtering the Data",
    "text": "Cleaning and Filtering the Data\nIn our cancer.csv file, some data is not relevant to my analysis. I am only interested in specific cancer types and known races, so I will exclude entries where the cancer type is “All Invasive Cancer Sites Combined” and where the race is “Other Races and Unknown Combined”. This allows me to focus solely on the cancers and races of interest."
  },
  {
    "objectID": "blog.html#filtering-the-data",
    "href": "blog.html#filtering-the-data",
    "title": "Data Science Blog: Working with Data in Pandas Tutorial",
    "section": "Filtering the Data",
    "text": "Filtering the Data\nSometimes, we might work with data that is not relevant to our analysis. For this data set, let’s say we are only interested in specific cancer types and known races, so we will exclude entries where the cancer type is “All Invasive Cancer Sites Combined” and where the race is “Other Races and Unknown combined”. This allows us to focus on the cancers and races of interest.\nAdditionally, the dataset contains a “Notes” column that does not contain any useful data, so we will remove it.\nNote that spelling is crucial in this step. In this dataset, “All Invasive Cancer Sites Combined” uses an uppercase “C” in “Combined,” whereas “Other Races and Unknown combined” uses a lowercase “c.”\nClick on the dropdown arrow beside the word codebelow to see the code and comments that exolain how the code works. You will need to do this for the rest of the tutorial:\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"cancer.csv\")\n\n# Filter the rows to exclude \"All Invasive Cancer Sites Combined\" and \"Other Races and Unknown combined\"\ndf_filtered = df[\n    (df['Cancer Sites'] != \"All Invasive Cancer Sites Combined\") &\n    (df['Race'] != \"Other Races and Unknown combined\")\n]\n\n# eliminate the Notes column using the .drop() method\ndf_filtered = df_filtered.drop(columns=['Notes'])\n\n# eliminate rows where all remaining values are NaN\ndf_filtered = df_filtered.dropna(how='all')\n\n# Display the filtered data\ndf_filtered\n\n\n\n\n\n\n\n\n\nRace\nRace Code\nCancer Sites\nCancer Sites Code\nAge Groups\nAge Groups Code\nSex\nSex Code\nCount\n\n\n\n\n38\nAmerican Indian or Alaska Native\n1002-5\nOral Cavity and Pharynx\n20010-20100\n20-24 years\n20-24\nFemale\nF\n16.0\n\n\n39\nAmerican Indian or Alaska Native\n1002-5\nOral Cavity and Pharynx\n20010-20100\n20-24 years\n20-24\nMale\nM\n19.0\n\n\n40\nAmerican Indian or Alaska Native\n1002-5\nOral Cavity and Pharynx\n20010-20100\n25-29 years\n25-29\nFemale\nF\n26.0\n\n\n41\nAmerican Indian or Alaska Native\n1002-5\nOral Cavity and Pharynx\n20010-20100\n25-29 years\n25-29\nMale\nM\n28.0\n\n\n42\nAmerican Indian or Alaska Native\n1002-5\nOral Cavity and Pharynx\n20010-20100\n30-34 years\n30-34\nFemale\nF\n32.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10420\nWhite\n2106-3\nMale Breast, In Situ\nBreast-InSitu-Male\n65-69 years\n65-69\nMale\nM\n571.0\n\n\n10421\nWhite\n2106-3\nMale Breast, In Situ\nBreast-InSitu-Male\n70-74 years\n70-74\nMale\nM\n511.0\n\n\n10422\nWhite\n2106-3\nMale Breast, In Situ\nBreast-InSitu-Male\n75-79 years\n75-79\nMale\nM\n443.0\n\n\n10423\nWhite\n2106-3\nMale Breast, In Situ\nBreast-InSitu-Male\n80-84 years\n80-84\nMale\nM\n282.0\n\n\n10424\nWhite\n2106-3\nMale Breast, In Situ\nBreast-InSitu-Male\n85+ years\n85\nMale\nM\n167.0\n\n\n\n\n10273 rows × 9 columns"
  },
  {
    "objectID": "blog.html#plotting-with-pandas",
    "href": "blog.html#plotting-with-pandas",
    "title": "Data Science Blog: Working with Data in Pandas Tutorial",
    "section": "Plotting with Pandas",
    "text": "Plotting with Pandas\nNow that we have our data cleaned and ready, it is time to explore some of Pandas’ visualization tools. In this tutorial, we will specifically look at the following plots:\n\nBar charts\n\nStacked bar charts\n\nHeatmaps\n\nPie charts\n\nScatter plots\n\n\n1. Bar Charts\nBar charts are ideal for grouping cancer counts by variables such as cancer site or race. In Pandas, we can group data, sum the counts, and plot it. The kind parameter specifies the type of chart:\n\n\nCode\ndf_filtered.groupby('Cancer Sites')['Count'].sum().sort_values().plot(kind='bar')\n\n\n\n\n\n\n\n\n\nThis graph is hard to interpret because the dataset contains many cancer sites. To make it more readable, we can focus on a subset of cancer sites. I chose: Stomach, Lung and Bronchus, Soft Tissue including Heart, Female Breast, Prostate, Kidney and Renal Pelvis, Brain, and Leukemias.\nHere is the code for that, with accompanying comments. Notice how we create a specific list of sites we want to include and then use the .isin() method to filter for them when we generate the plot:\n\n\nCode\n# List of specific cancer sites\nselected_sites = [\n    \"Stomach\",\n    \"Lung and Bronchus\",\n    \"Soft Tissue including Heart\",\n    \"Female Breast\",\n    \"Prostate\",\n    \"Kidney and Renal Pelvis\",\n    \"Brain\",\n    \"Leukemias\"\n]\n\n# Plot only the selected sites using the  .isin() methid\ndf[df['Cancer Sites'].isin(selected_sites)].groupby('Cancer Sites')['Count'].sum().sort_values().plot(kind='bar')\n\n\n\n\n\n\n\n\n\nFrom this chart, we can easily see that certain cancers—such as Lung, Prostate, and Female Breast—are much more prevalent than others like Stomach or Brain cancer.\n\n\n2. Stacked Bar Charts\nWhen we want to visualize how cancer counts are distributed across different groups, such as sex, age, or race, stacked bar charts are useful.\nWhen we make stacked bar charts, we can make use of the .pivot_table() method. In pandas, a pivot table is similiar to it’s Excel counterpart in that it is a way to summarize and aggregate data in a table format by specifying which columns to use as rows, which to use as columns, and which values to aggregate.\n\n\nCode\n# List of specific cancer sites, expanded\nselected_sites = [\n    \"Stomach\",\n    \"Lung and Bronchus\",\n    \"Soft Tissue including Heart\",\n    \"Female Breast\",\n    \"Prostate\",\n    \"Kidney and Renal Pelvis\",\n    \"Brain\",\n    \"Leukemias\",\n    \"Oral Cavity and Pharynx\",\n    \"Salivary Gland\",\n    \"Esophagus\",\n    \"Liver and Intrahepatic Bile Duct\",\n    \"Melanoma of the Skin\",\n    \"Non-Hodgkin Lymphoma\",\n    \"Eye and Orbit\",\n    \"Endocrine System\",\n    \"Thyroid\"\n]\n\n# Filter for the selected cancer sites using .isin()\ndf_selected = df_filtered[df_filtered['Cancer Sites'].isin(selected_sites)]\n\n# Pivot table: Cancer Sites x Sex\ndf_pivot_sex = df_selected.pivot_table(\n    index='Cancer Sites', # we want cancer sites to be the rows\n    columns='Sex', ## we want sex to be the columns\n    values='Count', ## these are the values we want to aggregate\n    aggfunc='sum' ## we will aggregate the data by summing them\n)\n\n# Plot stacked bar chart\ndf_pivot_sex.plot(kind='bar', stacked=True)\n\n\n\n\n\n\n\n\n\nThis type of chart allows us to analyze patterns by sex. For example, Prostate cancer occurs only in biological males, and Female Breast cancer occurs only in biological females. For other cancers, such as Lung, the counts are similar for both sexes. Cancers of the endocrine system and thyroid appear more prevalent in males, whereas oral cavity and kidney cancers appear to be more common in females.\n\n\n3. Heatmaps\nHeatmaps are a powerful way to visualize patterns among two categorical variables. For this data, let’s use a heatmap to show which cancer sites have higher counts for different age groups. This makes it easy to spot trends at a glance.\nIn order to make heatmaps in Pandas, we will need to use the python visualization library matplotlib.\nFirst, let’s install matblotlib. In your terminal, run: pip install matplotlib.\nNext, run the following code to generate a heatmap:\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# List of selected cancer sites to make the heatmap easier to read\nselected_sites = [\n    \"Stomach\",\n    \"Lung and Bronchus\",\n    \"Soft Tissue including Heart\",\n    \"Female Breast\",\n    \"Prostate\",\n    \"Kidney and Renal Pelvis\",\n    \"Brain\",\n    \"Leukemias\",\n    \"Oral Cavity and Pharynx\",\n    \"Salivary Gland\",\n    \"Esophagus\",\n    \"Liver and Intrahepatic Bile Duct\",\n    \"Melanoma of the Skin\",\n    \"Non-Hodgkin Lymphoma\",\n    \"Eye and Orbit\",\n    \"Endocrine System\",\n    \"Thyroid\"\n]\n\n# Filter the DataFrame to only look at the sites we are interested in using the isin() method\ndf_selected = df_filtered[df_filtered['Cancer Sites'].isin(selected_sites)]\n\n#Make a pivot table that makes it so that the data frame only contains the information we need.\ndf_pivot = df_selected.pivot_table(\n    index='Cancer Sites', \n    columns='Age Groups', \n    values='Count', \n    aggfunc='sum'\n)\n\n# Create the heatmap. Use the color red.\nplt.imshow(df_pivot, cmap='Reds')\n\n# Add x and y axis labels; rotate the x labels 90 degress so that they don't overlap one another. \nplt.xticks(range(len(df_pivot.columns)), df_pivot.columns, rotation=90)\nplt.yticks(range(len(df_pivot.index)), df_pivot.index)\n\n# Add a colorbar to show the scale\nplt.colorbar(label=\"Count\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAs you can see, the dark red areas correspond to higher counts. For example, Female Breast cancer, Lung and Bronchus cancer, and Prostate cancer are most prevalent in the 50–85 year age range, with the highest number of Prostate cancer cases occurring in the 65–69 year group.\n\n\n4. Pie Charts\nPie charts are useful for showing how a total count is divided among categories, such as Male vs Female cases for a specific cancer type. They provide a quick visual sense of which group accounts for a larger proportion of cases.\nWith Pandas, you can create a pie chart directly using the plot(kind='pie') method — no need to call matplotlib functions.\nHere is some code that shows how Liver and Intrahepatic Bile Duct cancer is distributed across the sexes:\n\n\nCode\n# Filter for Liver and Intrahepatic Bile Duct cancer\ndf_liver = df_selected[df_selected['Cancer Sites'] == \"Liver and Intrahepatic Bile Duct\"]\n\n# Aggregate counts by sex\ncounts_by_sex = df_liver.groupby('Sex')['Count'].sum()\n\n# Create the pie chart using Pandas\ncounts_by_sex.plot(\n    kind='pie',        # specify a pie chart\n    autopct='%1.1f%%', # show percentages to one decimal point\n)\n\n\n\n\n\n\n\n\n\n\n\n5. Scatter Plots\nThe last graph we’ll look at is a scatter plot, which shows the relationship between two numbers by plotting each data point as a dot. Here, we’ll use it to see how Liver and Intrahepatic Bile Duct cancer counts are spread across different age groups: &lt;1 year, 1–4 years, …, 80–84 years, and 85+ years.\n\n\nCode\n# Make the age categories into numbers first\nages = {\n    \"&lt; 1 year\": 0,\n    \"1-4 years\": 2.5,\n    \"5-9 years\": 7,\n    \"10-14 years\": 12,\n    \"15-19 years\": 17,\n    \"20-24 years\": 22,\n    \"25-29 years\": 27,\n    \"30-34 years\": 32,\n    \"35-39 years\": 37,\n    \"40-44 years\": 42,\n    \"45-49 years\": 47,\n    \"50-54 years\": 52,\n    \"55-59 years\": 57,\n    \"60-64 years\": 62,\n    \"65-69 years\": 67,\n    \"70-74 years\": 72,\n    \"75-79 years\": 77,\n    \"80-84 years\": 82,\n    \"85+ years\": 90\n}\n\n## use on the data where the cancer site is liver cancer. Don't chage the original data, just take a copy. \ndf_liver = df_selected[df_selected['Cancer Sites'] == \"Liver and Intrahepatic Bile Duct\"].copy()\n\n## Replace each age range in the data frame with a numberic value that represents that age range.\ndf_liver['Age Groups'] = df_liver['Age Groups'].map(ages)\n\n# create the scatter plot with the following axes\ndf_liver.plot.scatter(x='Age Groups', y='Count')\n\n\n\n\n\n\n\n\n\nThis scatterplot shows cancer case counts by age. The vertical spread at each age reflects differences between demographic groups and cancer types, but the clear overall trend is that liver cancer incidence increases sharply with age.\n## Going Further with Graphing\nIn this tutorial, we covered how to plot data using pandas. When we introduced the heatmap, we mentioned the Python library matplotlib. You can explore the wide variety of plot types available in matplotlib here.\nAdditionally, there are many ways to enhance your graphs, such as changing colors, adjusting axes, and more.\nHere is an example of how we might do that with a pie chart. Click on the code arrow to view the changes we made:\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Filter for Liver and Intrahepatic Bile Duct cancer\ndf_liver = df_selected[df_selected['Cancer Sites'] == \"Liver and Intrahepatic Bile Duct\"]\n\n# Aggregate counts by sex\ncounts_by_sex = df_liver.groupby('Sex')['Count'].sum()\n\n# Create the pie chart and get the Axes object\nax = counts_by_sex.plot(\n    kind='pie',\n    autopct='%1.1f%%',\n    colors=['pink', 'lightskyblue'],\n    ylabel=''  # removes default y-axis label\n)\n\n# Set the styled title\nax.set_title(\n    \"Liver and Intrahepatic Bile Duct Cancer by Sex\",\n    fontsize=16,\n    fontweight='bold',\n    color='mediumorchid'\n)\n\n# Show the plot cleanly\nplt.show()"
  },
  {
    "objectID": "blog.html#plotting-with-pandas-1",
    "href": "blog.html#plotting-with-pandas-1",
    "title": "Data Science Blog: Working with Data in Pandas Tutorial",
    "section": "Plotting with Pandas",
    "text": "Plotting with Pandas\nNow that we have our data cleaned and ready, it is time to explore some of Pandas’ visualization tools. In this tutorial, we will specifically look at the following plots:\n\nBar charts\n\nStacked bar charts\n\nBox plots\n\nHeat maps\n\nPie charts\n\nScatter plots"
  },
  {
    "objectID": "blog.html#conclusion",
    "href": "blog.html#conclusion",
    "title": "Data Science Blog: Working with Data in Pandas Tutorial",
    "section": "Conclusion",
    "text": "Conclusion\nI hope this tutorial helped you understand how to use the Python library pandas to clean and filter a dataset, as well as how to use different types of plots to visualize and analyze the data.\nNow it’s your turn! Find a dataset and apply these techniques to it. You can explore great datasets on Data.gov or use your own data. If you want to learn more about pandas, the documentation can be found on this website.\n\n\n\nPython Pandas Logo"
  },
  {
    "objectID": "blog.html#going-futher-with-graphing",
    "href": "blog.html#going-futher-with-graphing",
    "title": "Data Science Blog: Working with Data in Pandas Tutorial",
    "section": "Going Futher with Graphing",
    "text": "Going Futher with Graphing"
  },
  {
    "objectID": "about.html#get-to-know-me",
    "href": "about.html#get-to-know-me",
    "title": "About Me",
    "section": "",
    "text": "I am an Applied Statistics major at Brigham Young University. I was born and raised in Canada, but I have also lived in Japan, the Philippines, and the United States. I am happily married to my beautiful wife, and we have a 4-month-old son named Leo. I love learning new things, and you can often find me reading. I have been training in jiujitsu since I was eleven years old, and I also enjoy skiing and spending time playing with my son."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nBYU Department of Mathematics | Sep 2025 – Present\nGrader – Provo, UT\n- Assist faculty by grading Calculus I, II, and III homework assignments and exams.\n- Provide written feedback on student work, communicate concerns to instructors, and publish grades to Canvas via Gradescope.\n- Develop and apply grading rubrics to ensure consistent and fair evaluation.\nPride Gym | Sep 2019 – Jul 2022\nJiu Jitsu Instructor – Trail, BC, Canada\n- Instructed youth ages seven through twelve to safely and efficiently use various Jiu Jitsu techniques.\n- Ensured the safety and well-being of youth and maintained a safe working environment.\n- Coached and supervised youth in local competitions.\nMcDonald’s Canada | Aug 2020 – Jan 2022\nCrew Trainer – Trail, BC, Canada\n- Trained new hires on procedures and policies.\n- Took orders from guests, made beverages, and prepared orders.\n- Handled cash, credit card, and debit transactions.\n- Greeted customers in a friendly manner and ensured their needs were met.\n- Handled customer complaints calmly and professionally.\nThe Church of Jesus Christ of Latter-day Saints | Jul 2022 – Dec 2023\nJapanese Speaking Volunteer – Chūbu Region, Japan\n- Led groups of 10–12 volunteers, conducted weekly training meetings, followed up on goals, and created weekly progress reports.\n- Taught beginning and intermediate level weekly English as a second language classes based on the EnglishConnect curriculum.\n- Engaged in diverse service projects, including disaster relief, vineyard work, gardening, and assisting with relocations.\n- Delivered presentations and taught lessons in Japanese.\n- Used Canva to design graphics and advertisements and Excel for record-keeping."
  },
  {
    "objectID": "projects/data-acquisition.html#introduction",
    "href": "projects/data-acquisition.html#introduction",
    "title": "Data Acquisition Blog",
    "section": "",
    "text": "This blog post showcases how I used the Python package Beautiful Soup to curate a custom dataset. Finding a website to scrape can be challenging, as many sites with interesting data—such as Rate My Professors and other review platforms—disallow scraping.\nI ultimately decided to scrape the Book of Mormon, found on the Church of Jesus Christ of Latter-day Saints’ website. My goal was to gather data on each chapter, including the chapter summary, the average verse length per chapter, and the most frequent word in each chapter. I thought this would be interesting both as a personal project and as an example for others wanting to learn how to use Beautiful Soup to scrape other HTML-based texts.",
    "crumbs": [
      "Data Acquisition Blog"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#ethics",
    "href": "projects/data-acquisition.html#ethics",
    "title": "Data Acquisition Blog",
    "section": "Ethics",
    "text": "Ethics\nBefore scraping any website, it’s important to review the site’s robots.txt file—a plain text file that provides instructions about which pages may or may not be crawled or scraped.\nThe Church of Jesus Christ of Latter-day Saints’ robots.txt disallows scraping of certain parts of its website, such as /scriptures/search and /study/manual/handbook-1, but does not prohibit access to /study/scriptures/bofm?lang=eng, which is the section containing the Book of Mormon. See below for a portion of the robots.txt file:\n\n\n\nrobots.txt for the Church of Jesus Christ of Latter-day Saints’ website\n\n\nThere was also no rate limit or time-delay restriction specified between requests. Based on this information, I determined that scraping this data was ethically acceptable.",
    "crumbs": [
      "Data Acquisition Blog"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#how-the-data-was-gathered",
    "href": "projects/data-acquisition.html#how-the-data-was-gathered",
    "title": "Data Acquisition Blog",
    "section": "How the Data Was Gathered",
    "text": "How the Data Was Gathered\nFor this project, I used the following Python libraries:\n\nrequests for making HTTP requests.\n\nBeautifulSoup for xtracting data from web pages.\n\npandas for analyzing and manipulating data.\n\nre to search, match, and manipulate text using regular expressions.\n\ncollections.Counter, a tool I hadn’t used before, which counts the occurrences of items in a list or sequence. This was helpful for finding the most common word in each chapter.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom collections import Counter\nimport re\nMany words appear frequently but are not meaningful for analysis, including prepositions, transition words, and other common words in older texts. I labeled these as trivial_words in my code. This allowed me to focus on the interesting, non-trivial words that were most common.\nTo loop through each book in the Book of Mormon, I created a list called books and a base_url to append each book to, as shown below:\n\nbooks = [\n    \"1-ne\", \"2-ne\", \"jacob\", \"enos\", \"jarom\", \"omni\", \"w-of-m\",\n    \"mosiah\", \"alma\", \"hel\", \"3-ne\", \"4-ne\", \"morm\", \"ether\", \"moro\"\n]\n\nbase_url = \"https://www.churchofjesuschrist.org/study/scriptures/bofm\"\nUsing the requests library with UTF-8 character encoding (to avoid strange characters like â€”), I inspected the HTML to locate the content of interest:\nChapter summaries in class_=\"study-summary\"\nScripture verses in class_=\"verse\"\nI also removed footnotes to get plain text by targeting the class_=\"study-note-ref\" elements and using the decompose() method to delete them.\nThis process was repeated for each chapter in each Book of Mormon book if the number of verses was greater than zero. If a chapter did not exist, the URL redirected to the book’s main page (showing all chapters and summaries with no verses), which allowed the code to break out of the loop and move to the next book.\nOnce all the data was gathered, it was compiled into a pandas DataFrame that looked like the following table:\n\n\n\n\n\n\n\n\n\n\n\nBook\nChapter\nSummary\nNum Verses\nMost Common Word\nAvg Words per Verse\n\n\n\n\n1 Nephi\n1\nNephi begins the record of his people …\n20\nfather\n43.9\n\n\n1 Nephi\n2\nLehi takes his family …\n24\nlord\n35.29\n\n\n1 Nephi\n3\nLehi’s sons return to Jerusalem …\n31\nlord\n34.19\n\n\n1 Nephi\n4\nNephi slays Laban …\n38\nlaban\n33.05",
    "crumbs": [
      "Data Acquisition Blog"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#exploratory-data-analysis",
    "href": "projects/data-acquisition.html#exploratory-data-analysis",
    "title": "Data Acquisition Blog",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nNow that we have a dataset, we can do some EDA. Click on the arrow beside the word code to view the code chunks for each anaylsis\nWe might want to find how many oberservations (rows) the data frame has:\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"book_of_mormon_dataset.csv\")\nlen(df)\n\n\n239\n\n\nWe might want to know the top 5 most common non-trivial words that show up in the Book of Mormon:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntop_words = df['most_common_word'].value_counts().head(5)\n\nplt.figure(figsize=(8,5))\ntop_words.plot(kind='bar', color='skyblue')\nplt.title(\"Top 5 Most Common Words in Chapters of the Book of Mormon\")\nplt.xlabel(\"Word\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=45)\nplt.ylim(0, 50)\nplt.show()\n\n\n\n\n\n\n\n\n\nIt is interesting that people is the most common word non-trivial word per chapter, showing up over\nWe can also find some summary statistics about the data:\n\n\nCode\nmean_verses = df['num_verses'].mean()\nstd_verses = df['num_verses'].std()\nprint(f\"Mean number of verses per chapter is {mean_verses:.2f} with a standard deviation of {std_verses:.2f}\")\n\nmean_words = df['avg_words_per_verse'].mean()\nstd_words = df['avg_words_per_verse'].std()\nprint(f\"The mean words per verse is {mean_words:.2f} with a standard deviation of {std_words:.2f}\")\n\n\nMean number of verses per chapter is 27.63 with a standard deviation of 11.92\nThe mean words per verse is 39.85 with a standard deviation of 6.70\n\n\nA histogram of the average words per verse looks like:\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,5))\nplt.hist(df['avg_words_per_verse'], bins=15, color='lightgreen', edgecolor='black')\nplt.title(\"Distribution of Average Words per Verse\")\nplt.xlabel(\"Average Words per Verse\")\nplt.ylabel(\"Number of Chapters\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIs there a correclation between number of verses in a chapter and average words per verse. In other words, do longer chapters have longer verses? Lets see:\n\n\nCode\ndf[['num_verses', 'avg_words_per_verse']].corr()\n\n\n\n\n\n\n\n\n\nnum_verses\navg_words_per_verse\n\n\n\n\nnum_verses\n1.000000\n0.010976\n\n\navg_words_per_verse\n0.010976\n1.000000\n\n\n\n\n\n\n\nIt looks like there’s almost no linear relationship between chapter length (number of verses) and the average number of words per verse. In other words, longer chapters don’t necessarily have longer verses, and shorter chapters don’t necessarily have shorter verses.\nThis is easily visualized by the scatterplot below:\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,5))\nplt.scatter(df['num_verses'], df['avg_words_per_verse'], alpha=0.6)\nplt.title(\"Num Verses vs Avg Words per Verse\")\nplt.xlabel(\"Number of Verses\")\nplt.ylabel(\"Average Words per Verse\")\nplt.show()",
    "crumbs": [
      "Data Acquisition Blog"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#exploratory-data-analysis-eda",
    "href": "projects/data-acquisition.html#exploratory-data-analysis-eda",
    "title": "Data Acquisition Blog",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nNow that we have a dataset, we can perform some Exploratory Data Analysis (EDA). Click on the arrow beside the word code to view the code chunks for each analysis.\n\n\n1. Dataset Dimensions\nWe want to find out how many observations (rows) the dataframe contains:\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"book_of_mormon_dataset.csv\")\n# Output: The number of rows in the DataFrame\nlen(df) \n\n\n239\n\n\n\n\n\n2. Top 5 Most Common Words\nWe want to know the top 5 most common non-trivial words that show up in the Book of Mormon:\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntop_words = df['most_common_word'].value_counts().head(5)\n\nplt.figure(figsize=(8,5))\ntop_words.plot(kind='bar', color='skyblue')\nplt.title(\"Top 5 Most Common Words in Chapters of the Book of Mormon\")\nplt.xlabel(\"Word\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=45)\nplt.ylim(0, 50)\nplt.show()\n\n\n\n\n\n\n\n\n\nIt’s interesting that “people” is the most common non-trivial word per chapter, appearing nearly 50 times. Being a religious text, it makes sense that “Lord” and “God” are the next most common. I find it insightful that “behold” was relatively common, suggesting that the Book of Mormon emphasizes the importance of listening to God and His commandments.\n\n\n\n3. Summary Statistics\nWe can also find some summary statistics about the data:\n\n\nCode\nmean_verses = df['num_verses'].mean()\nstd_verses = df['num_verses'].std()\nprint(f\"Mean number of verses per chapter is {mean_verses:.2f} with a standard deviation of {std_verses:.2f}\")\n\nmean_words = df['avg_words_per_verse'].mean()\nstd_words = df['avg_words_per_verse'].std()\nprint(f\"The mean words per verse is {mean_words:.2f} with a standard deviation of {std_words:.2f}\")\n\n\nMean number of verses per chapter is 27.63 with a standard deviation of 11.92\nThe mean words per verse is 39.85 with a standard deviation of 6.70\n\n\nA histogram of the average words per verse looks like:\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,5))\nplt.hist(df['avg_words_per_verse'], bins=15, color='lightgreen', edgecolor='black')\nplt.title(\"Distribution of Average Words per Verse\")\nplt.xlabel(\"Average Words per Verse\")\nplt.ylabel(\"Number of Chapters\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n4. Correlation Analysis: Verse Length\nIs there a correlation between the number of verses in a chapter and the average words per verse? In other words, do longer chapters have longer verses? Let’s see:\n\n\nCode\n# Output: Correlation matrix for the two columns\ndf[['num_verses', 'avg_words_per_verse']].corr() \n\n\n\n\n\n\n\n\n\nnum_verses\navg_words_per_verse\n\n\n\n\nnum_verses\n1.000000\n0.010976\n\n\navg_words_per_verse\n0.010976\n1.000000\n\n\n\n\n\n\n\nThe result indicates there’s almost no linear relationship between chapter length (number of verses) and the average number of words per verse. This means longer chapters don’t necessarily have longer verses, and shorter chapters don’t necessarily have shorter verses.\nThis is easily visualized by the scatterplot below:\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8,5))\nplt.scatter(df['num_verses'], df['avg_words_per_verse'], alpha=0.6)\nplt.title(\"Number of Verses vs. Average Words per Verse\")\nplt.xlabel(\"Number of Verses\")\nplt.ylabel(\"Average Words per Verse\")\nplt.show()",
    "crumbs": [
      "Data Acquisition Blog"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#conclusion",
    "href": "projects/data-acquisition.html#conclusion",
    "title": "Data Acquisition Blog",
    "section": "Conclusion",
    "text": "Conclusion\nI hope that this blog was able to help you understand a little bit more about the web scraping process. My code that I used to scrape the Book of Mormon, as well as the CSV file that it curated, can be found at: this GitHub repository\nTry to scrape your own HTML-based book! Some possible options include: The New Testament and Hamlet, but make sure you read the robots.txt of any website to determine whether or not you should scrape it.\nFurther resources for learning more about web scraping can be found below:\n\nRequests Documentation\nBeautiful Soup Documentation\nBeautiful Soup Crash Course (YouTube)",
    "crumbs": [
      "Data Acquisition Blog"
    ]
  }
]